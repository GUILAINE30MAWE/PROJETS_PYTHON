{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "norwegian-sandwich",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "excess-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = \"https://www.usine-digitale.fr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "popular-december",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette fonction permet de recupérer tout le texte qui se retrouve dans une balise. \n",
    "def get_text (soup_obj) :\n",
    "    all_string=\"\"\n",
    "    for node in soup_obj:\n",
    "        all_string = all_string + ''.join(node.findAll(text=True))\n",
    "    return all_string.replace(\"\\n\", \"\").replace(\"\\t\", \"\").replace(\"\\r\", \" \").strip()\n",
    "\n",
    "\n",
    "\n",
    "# Cette fonction permet de récupérer les données \n",
    "def get_all_data_companies (link): \n",
    "\n",
    "    r_page = requests.get(link) \n",
    "    soup_page = BeautifulSoup(r_page.text, 'html.parser')\n",
    "        \n",
    "    # On récupère toutes les balises <a> qui contiennent les liens des entreprises\n",
    "    results_details_links = soup_page.select('section.blocType1 a.contenu')\n",
    "    \n",
    "        \n",
    "    links_company =  [] # Contiendra les liens qu'on aura extrait des balises <a> précédentes\n",
    "    for i in range (0, len(results_details_links)) :\n",
    "    \n",
    "        # On extrait le lien dans l'attribut \"href\" de chaque balise <a> et on ajoute la base : \"https://www.usine-digitale.fr\"\n",
    "        link = url_base+results_details_links[i][\"href\"].strip()\n",
    "        \n",
    "        # On injecte le lien dans le tableau\n",
    "        links_company.append(link)\n",
    "    \n",
    "    list_companies_data =[] \n",
    "    # On recupère les données dans chaque page de detail d'une entreprise\n",
    "    for j in range (0, len(links_company)):\n",
    "        \n",
    "        r_company_details = requests.get(links_company[j])\n",
    "\n",
    "        # On formate en type html\n",
    "        soup_company_details = BeautifulSoup(r_company_details.text, \"html.parser\")\n",
    "        \n",
    "        # On extrait les données de l'enteprise séleectionnée\n",
    "        try:#### Test des opérations\n",
    "            name = get_text(soup_company_details.select(\"h1[itemprop = name]\"))\n",
    "            presentation =get_text(soup_company_details.select(\"div[itemprop = description]\"))\n",
    "            dateCreation =get_text(soup_company_details.select(\"meta[itemprop = foundingDate] + p\"))\n",
    "            email = get_text(soup_company_details.select(\"p[itemprop = email]\"))\n",
    "            address  = get_text(soup_company_details.select(\"p[itemprop = address]\"))\n",
    "            telephone = get_text(soup_company_details.select(\"p[itemprop = telephone]\"))\n",
    "            genese =get_text(soup_company_details.select(\"div[itemprop = review] > p\"))\n",
    "            produits =get_text(soup_company_details.select(\"div[itemprop = makesOffer] > p\"))\n",
    "            createur =get_text(soup_company_details.select(\"div[itemprop = founders] > p\"))\n",
    "            implantation =get_text(soup_company_details.select(\"div[itemprop = address] > p\"))\n",
    "            \n",
    "            list_companies_data.append((name,presentation,dateCreation,email,address,telephone,genese,produits,createur,implantation))\n",
    "            \n",
    "        except: # S'il y a un problème, on passe à la prochaine valeur\n",
    "            continue\n",
    "    \n",
    "    # Création du fichier dataset\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(list_companies_data, columns=[\"name\",\"presentation\",\"dateCreation\",\"email\",\"address\",\"telephone\",\"genese\",\"produits\",\"createur\",\"implantation\"])\n",
    "    df.to_csv('companies_data.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "voluntary-pierre",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url_base+\"/annuaire-start-up\") # Récupération de la page principale qui contient les annuaires\n",
    "\n",
    "soup = BeautifulSoup(r.text, 'html.parser') # On formate le contenu html de type texte vers le type html\n",
    "\n",
    "nb_page = 2 # Nombre de pages souhaitées (maxi 9)\n",
    "\n",
    "results = soup.select('ul.pagination li a') # Récupération des balises qui contiennent les liens de pagination\n",
    "\n",
    "if (nb_page <= 9 and nb_page >=0):\n",
    "    \n",
    "    # On parcours les liens de pagination\n",
    "    for i in range (0, nb_page) :\n",
    "        link_page = url_base+results[i][\"href\"].strip() # lien de la page courante\n",
    "\n",
    "        get_all_data_companies(link_page) # Récupération des données de chaque page\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-seeker",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "catholic-carrier",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-tamil",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
